{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Easy Naive Bayes model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Python notebook is about a simple Naive Bayes model implementation applied to The Spam Filtering problem. The dataset used is the <b> SpamAssassin public corpus </b> (https://spamassassin.apache.org/old/publiccorpus/, look for the three files under the prefix 20021010).   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the model, after extracting the data you should have three folders: <b> spam</b>, <b>easy_ham</b>, and <b>hard_ham</b>. Each folder contains many emails, each contained in a single file. To keep things really simple, we’ll just look at the subject lines of each email."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s create a simple function to tokenize messages into distinct words. We’ll first convert each message to lowercase, then we'll use `re.findall()` to extract “words” (strings) consisting of letters, numbers, and apostrophes. Finally we will use `set()` to get just the distinct words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(message):\n",
    "    message = message.lower()                     # convert to lowercase\n",
    "    all_words = re.findall(\"[a-z0-9']+\", message) # extract the words\n",
    "    \n",
    "    return set(all_words)                         # remove duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function will count the words in a set of labeled messages (the training set) and it will return a dictionary where each key-value pair has the form `(word, [spam_count, non_spam_count])`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def count_words(training_set):\n",
    "    \"\"\"training set consists of pairs (message, is_spam)\"\"\"\n",
    "    counts = defaultdict(lambda: [0, 0])\n",
    "    for message, is_spam in training_set:\n",
    "        for word in tokenize(message): # for each word\n",
    "            # increase the word count according to \n",
    "            # spam/non-spam condition\n",
    "            counts[word][0 if is_spam else 1] += 1\n",
    "    \n",
    "    return counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Naive Bayes assumption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider am email message $E$ containing words $w_1,\\dots,w_n$. The probability of receiving the email $E$ is equal to the probability of receiving the list of words $w_1, \\dots, w_n$\n",
    "\n",
    "$$\\mathrm{Pr}(w_1,\\dots,w_n)\\,.$$\n",
    "\n",
    "This probability is impossible to evaluate in practice because if the email contains a large number of words, then one needs a huge training set containing all the possible combinations of the probabilities of words in the email. Even if we had a reasonably large dataset, it would be still very unlikely for us to find all the possible combinations everytime we receive a new email. But we can roughly overcome this issue if we consider the following (very strong) <b>assumption</b>:\n",
    "\n",
    "$$\\mathrm{Pr}(w_1,\\dots,w_n) = \\mathrm{Pr}(w_1) \\cdots \\mathrm{Pr}(w_n) \\,.$$\n",
    "\n",
    "This means that the words in the email are <b>independent</b> of each other and randomly displayed in the email. Despite this assumption is clearly <b>false</b> in most real-world cases, Naive Bayes often performs classification very well.\n",
    "\n",
    "Let's denote two classes of emails as spam $S$ and non-spam $\\neg S$. Let $\\mathrm{Pr}(E \\mid S)$ be the probability that a given email from class $S$ is the email $E$ and let $\\mathrm{Pr}(E \\mid \\neg S)$ the probability that a given non-spam email is $E$. We can express $\\mathrm{Pr}(E \\mid S)$ as \n",
    "\n",
    "$$ \\mathrm{Pr}(E \\mid S) = \\mathrm{Pr}(w_1,\\dots,w_n \\mid S) = \\mathrm{Pr}(w_1 \\mid S) \\cdots \\mathrm{Pr}(w_n \\mid S)\\,.  $$\n",
    "\n",
    "This very strong assumption leads to an oversimplified model (for this reason the method is called \"naive\"). In practice, to avoid underflow (computers don't deal well with numbers too close to zero) we compute\n",
    "\n",
    "$$\n",
    "\\exp\\left[ \\log(p_1) + \\cdots + \\log(p_n) \\right]\n",
    "$$\n",
    "\n",
    "where $p_i =  \\mathrm{Pr}(w_i \\mid S)$.\n",
    "\n",
    "We use Bayes rule to compute the posterior probability of spam email given the overall probability of the sampling email; this is the crucial part of the entire classification:\n",
    "\n",
    "$$ \\mathrm{Pr}(S \\mid E) = \\frac{\\mathrm{Pr}(S) \\cdot \\mathrm{Pr}(E \\mid S) }{\\mathrm{Pr}(E)} = \\frac{\\mathrm{Pr}(S) \\cdot \\prod_{i}  \\mathrm{Pr}(w_i \\mid S)}{\\mathrm{Pr}(E)}\\,.$$\n",
    "\n",
    "An analogous formula holds for $\\mathrm{Pr}(\\neg S \\mid E)$. Furthermore, using Bayes formula we can write\n",
    "\n",
    "$$ \\mathrm{Pr}(S \\mid E) = \\frac{ \\mathrm{Pr}(S) \\mathrm{Pr}(E \\mid S)} {\\mathrm{Pr}(E)} = \\frac{ \\mathrm{Pr}(S) \\mathrm{Pr}(E \\mid S)} {\\mathrm{Pr}(S)\\mathrm{Pr}(E \\mid S) + \\mathrm{Pr}(\\neg S)\\mathrm{Pr}(E \\mid \\neg S)} \\,,$$\n",
    "\n",
    "and if we further assume that any message is equally likely to be spam or not-spam, so that $\\mathrm{Pr}(S) = \\mathrm{Pr}(\\neg S) = 0.5$, the formula simplifies to\n",
    "\n",
    "$$\n",
    " \\mathrm{Pr}(S \\mid E) = \\frac{ \\mathrm{Pr}(S) \\mathrm{Pr}(E \\mid S)} {\\mathrm{Pr}(E)} = \\frac{\\mathrm{Pr}(E \\mid S)} {\\mathrm{Pr}(E \\mid S) + \\mathrm{Pr}(E \\mid \\neg S)} \\,.\n",
    "$$\n",
    "\n",
    "We will use the last formula in Step 3.\n",
    "If we have a fair number of “training” messages labeled as spam and non-spam, an obvious first try is to estimate  $\\mathrm{Pr}( w_i \\mid S)$ simply as the <b>fraction</b> of spam messages containing the word $w_i$. Intuition is fine but there's a problem: imagine that in our training set the vocabulary word “info” only occurs in nonspam messages. Then we would get $\\mathrm{Pr}(\\,\\mathsf{info} \\mid S) = 0$. The result is that our Naive Bayes classifier would always assign spam probability $0$ to any message containing the word “info\", even a message like “info about Rolex watches.” To avoid this problem, we usually use some kind of <b>smoothing</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smoothing\n",
    "When computing the spam probabilities for $w_i$, we assume we also saw $k$ additional spams containing the word $w_i$ and $k $additional spams not containing the word. So, choose a count $k$ and estimate the probability of seeing the word $w_i$ in a spam as:\n",
    "\n",
    "$$\\mathrm{Pr}(w_i \\mid S) = \\frac{ k + \\mathsf{number\\;of\\;spams\\;containing\\;} w_i}{ 2k + \\mathsf{number\\;of\\;spams}}\\,.$$\n",
    "\n",
    "Analogously for $\\mathrm{Pr}(w_i \\mid \\neg S)$. \n",
    "\n",
    "Example: if “info” occurs in 0/48 spam documents, and if $k$ is $1$, we estimate\n",
    "\n",
    "$$ \\mathrm{Pr}\\,(\\,\\mathsf{info} \\mid S\\,) = \\frac{ 1 + 0}{2\\cdot1 + 48} = \\frac{1}{50} = 0.02 \\,,$$\n",
    "\n",
    "which allows our classifier to still assign some nonzero spam probability to messages containing the word “info\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: turn counts into probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next function aims to turn these counts into estimated probabilities using the smoothing we described before. The function will return a list of triplets containing each word, the probability of seeing that word in a spam message, and the probability of seeing that word in a nonspam message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_probabilities(counts, total_spams, total_non_spams, k=0.5):\n",
    "    \"\"\"turn the word_counts into a list of triplets \n",
    "       [w, Pr(w | spam), Pr(w | non-spam)]\"\"\"\n",
    "    \n",
    "    return [(w,\n",
    "            (spam + k) / (total_spams + 2 * k),\n",
    "            (non_spam + k) / (total_non_spams + 2 * k))\n",
    "            for w, (spam, non_spam) in counts.items()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: assign probabilities to messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the evaluated word probabilities to assign probabilities to messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def spam_probability(word_probs, message):\n",
    "    '''returns Pr(S|E)'''\n",
    "    # tokenize message\n",
    "    message_words = tokenize(message)\n",
    "    # initialize log probabilities\n",
    "    log_prob_if_spam = log_prob_if_not_spam = 0.0\n",
    "    # iterate through each word in our vocabulary\n",
    "    for word, prob_if_spam, prob_if_not_spam in word_probs:\n",
    "        # if *word* appears in the message,\n",
    "        # add the log probability of seeing it\n",
    "        if word in message_words:\n",
    "            log_prob_if_spam += math.log(prob_if_spam)\n",
    "            log_prob_if_not_spam += math.log(prob_if_not_spam)\n",
    "        # if *word* doesn't appear in the message\n",
    "        # add the log probability of _not_ seeing it\n",
    "        # which is log(1 - probability of seeing it)\n",
    "        else:\n",
    "            log_prob_if_spam += math.log(1.0 - prob_if_spam)\n",
    "            log_prob_if_not_spam += math.log(1.0 - prob_if_not_spam)\n",
    "\n",
    "    prob_if_spam = math.exp(log_prob_if_spam)\n",
    "    prob_if_not_spam = math.exp(log_prob_if_not_spam)\n",
    "    \n",
    "    return prob_if_spam / (prob_if_spam + prob_if_not_spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a classifier class encompassing all the function defined so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesClassifier:\n",
    "    \n",
    "    def __init__(self, k=0.5):\n",
    "        self.k = k\n",
    "        self.word_probs = []\n",
    "        \n",
    "    def train(self, training_set):\n",
    "        # count spam and non-spam messages\n",
    "        num_spams = len([is_spam\n",
    "                        for message, is_spam in training_set\n",
    "                        if is_spam])\n",
    "        num_non_spams = len(training_set) - num_spams\n",
    "        \n",
    "        # run training data through our \"pipeline\"\n",
    "        word_counts = count_words(training_set)\n",
    "        self.word_probs = word_probabilities(word_counts,\n",
    "                                            num_spams,\n",
    "                                            num_non_spams,\n",
    "                                            self.k)\n",
    "        \n",
    "    def classify(self, message):\n",
    "        \n",
    "        return spam_probability(self.word_probs, message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: removing subject identifier from lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Files starting with \"Subject:\" are subject lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import glob, re\n",
    "\n",
    "# define the path containing the files\n",
    "path = r\".\\*\\*\"\n",
    "data = []\n",
    "\n",
    "# glob.glob returns every filename that matches the wildcarded path\n",
    "for fn in glob.glob(path):\n",
    "    # set as spam if ham is not in the filename\n",
    "    is_spam = \"ham\" not in fn\n",
    "    \n",
    "    with open(fn, 'r', errors='ignore') as file:\n",
    "        for line in file:\n",
    "            if line.startswith(\"Subject:\"):\n",
    "                # remove the leading \"Subject: \" and keep the rest of the line\n",
    "                subject = re.sub(r\"^Subject: \", \"\", line).strip()\n",
    "                data.append((subject, is_spam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Re: New Sequences Window', False),\n",
       " ('[zzzzteana] RE: Alexander', False),\n",
       " ('[zzzzteana] Moscow bomber', False),\n",
       " (\"[IRR] Klez: The Virus That  Won't Die\", False),\n",
       " ('Re: Insert signature', False),\n",
       " ('Re: [zzzzteana] Nothing like mama used to make', False),\n",
       " ('Re: [zzzzteana] Nothing like mama used to make', False),\n",
       " ('[zzzzteana] Playboy wants to go out with a bang', False),\n",
       " ('Re: [zzzzteana] Nothing like mama used to make', False),\n",
       " ('[zzzzteana] Meaningful sentences', False)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6:  split the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the data into training data and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, prob):\n",
    "    \"\"\"split data into fractions [prob, 1 - prob]\"\"\"\n",
    "    results = [], []\n",
    "    for row in data:\n",
    "        results[0 if random.random() < prob else 1].append(row)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed(0) \n",
    "train_data, test_data = split_data(data, 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('[zzzzteana] Moscow bomber', False),\n",
       "  (\"[IRR] Klez: The Virus That  Won't Die\", False),\n",
       "  ('Re: Insert signature', False),\n",
       "  ('Re: [zzzzteana] Nothing like mama used to make', False),\n",
       "  ('[zzzzteana] Playboy wants to go out with a bang', False)],\n",
       " [('Re: New Sequences Window', False),\n",
       "  ('[zzzzteana] RE: Alexander', False),\n",
       "  ('Re: [zzzzteana] Nothing like mama used to make', False),\n",
       "  ('Re: New Sequences Window', False),\n",
       "  ('Re: New Sequences Window', False)])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:5], test_data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: testing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a classifier instance and then start training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = NaiveBayesClassifier()\n",
    "classifier.train(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# triplets (subject, actual is_spam, predicted spam probability)\n",
    "classified = [(subject, is_spam, classifier.classify(subject))\n",
    "                for subject, is_spam in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Re: New Sequences Window', False, 1.7492787736962358e-05),\n",
       " ('[zzzzteana] RE: Alexander', False, 4.36722330733585e-05),\n",
       " ('Re: [zzzzteana] Nothing like mama used to make',\n",
       "  False,\n",
       "  0.0006355823082923374),\n",
       " ('Re: New Sequences Window', False, 1.7492787736962358e-05),\n",
       " ('Re: New Sequences Window', False, 1.7492787736962358e-05),\n",
       " ('[ILUG] Re: Problems with RAID1 on cobalt raq3',\n",
       "  False,\n",
       "  0.0014660556345939412),\n",
       " ('Re: New Sequences Window', False, 1.7492787736962358e-05),\n",
       " ('The case for spam', False, 0.0017751881200852985),\n",
       " (\"[IIU] Eircom aDSL Nat'ing\", False, 0.06839623756971493),\n",
       " ('[zzzzteana] Which Muppet Are You?', False, 0.03531988332866546)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classified[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# assume that spam_probability > 0.5 corresponds to spam prediction\n",
    "# and count the combinations of (actual is_spam, predicted is_spam)\n",
    "counts = Counter((is_spam, spam_probability > 0.5)\n",
    "            for _, is_spam, spam_probability in classified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({(False, False): 704,\n",
       "         (False, True): 33,\n",
       "         (True, True): 101,\n",
       "         (True, False): 38})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We count 101 true positives (spam classified as “spam”), 33 false positives (ham classified as “spam”), 704 true negatives (ham classified as “ham”), and 38 false negatives (spam classified as “ham”). <b>Precision</b> score is \n",
    "\n",
    "$$ \\frac{\\mathsf{true\\;positives}}{\\mathsf{true\\;positives} + \\mathsf{false\\;positives}} = \\frac{101} {101 + 33} = 75\\%$$\n",
    "\n",
    "and <b>recall</b> score is \n",
    "\n",
    "$$ \\frac{\\mathsf{true\\;positives}}{\\mathsf{true\\;positives} + \\mathsf{false\\;negatives}} = \\frac{101}{101 + 38} = 73\\% \\,,$$\n",
    "\n",
    "really not bad for such a simple model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most misclassified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can sort messages by spam probability from smallest to largest using `sort`. The `row[2]` in sorting criterion refers to the probability component of a classified row of the form `[message, truth value, probability]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Re[2]: Selling Wedded Bliss (was Re: Ouch...)',\n",
       "  False,\n",
       "  3.048747404869583e-10),\n",
       " ('Re[2]: Selling Wedded Bliss (was Re: Ouch...)',\n",
       "  False,\n",
       "  3.048747404869583e-10),\n",
       " ('Re[2]: Selling Wedded Bliss (was Re: Ouch...)',\n",
       "  False,\n",
       "  3.048747404869583e-10),\n",
       " ('Re[2]: Selling Wedded Bliss (was Re: Ouch...)',\n",
       "  False,\n",
       "  3.048747404869583e-10),\n",
       " ('Re: [Razor-users] Problem with Razor 2.14 and Spamassassin 2.41',\n",
       "  False,\n",
       "  6.209324406814169e-10)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classified.sort(key=lambda row: row[2])\n",
    "classified[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The highest predicted spam probabilities among the non-spam messages probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Attn programmers: support offered [FLOSS-Sarai Initiative]',\n",
       "  False,\n",
       "  0.9756129605140706),\n",
       " ('2000+ year old Greek computer reinterpreted', False, 0.9835355008104817),\n",
       " ('What to look for in your next smart phone (Tech Update)',\n",
       "  False,\n",
       "  0.9898719206903398),\n",
       " ('[ILUG-Social] Re: Important - reenactor insurance needed',\n",
       "  False,\n",
       "  0.9995349057803387),\n",
       " ('[ILUG-Social] Re: Important - reenactor insurance needed',\n",
       "  False,\n",
       "  0.9995349057803387)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spammiest_hams = list(filter(lambda row: not row[1], classified))[-5:]\n",
    "spammiest_hams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lowest predicted spam probabilities among the actual spams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Re: girls', True, 0.0009525186158413646),\n",
       " ('Introducing Chase Platinum for Students with a 0% Introductory APR',\n",
       "  True,\n",
       "  0.0012566691211122248),\n",
       " ('.Message report from your contact page....//ytu855 rkq',\n",
       "  True,\n",
       "  0.0015109358288642688),\n",
       " ('Testing a system, please delete', True, 0.0026920538836931215),\n",
       " ('Never pay for the goodz again (8SimUgQ)', True, 0.005911623221945951)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hammiest_spams = list(filter(lambda row: row[1], classified))[:5]\n",
    "hammiest_spams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spammiest and hammiest words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_spam_given_word(word_prob):\n",
    "    \"\"\"uses Bayes's theorem to compute Pr(spam | message contains word)\"\"\"\n",
    "    # word_prob is one of the triplets produced by word_probabilities\n",
    "    word, prob_if_spam, prob_if_not_spam = word_prob\n",
    "    \n",
    "    return prob_if_spam / (prob_if_spam + prob_if_not_spam)\n",
    "\n",
    "words = sorted(classifier.word_probs, key=p_spam_given_word)\n",
    "\n",
    "spammiest_words = words[-5:]\n",
    "hammiest_words = words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('year', 0.028767123287671233, 0.00022893772893772894),\n",
       " ('rates', 0.031506849315068496, 0.00022893772893772894),\n",
       " ('sale', 0.031506849315068496, 0.00022893772893772894),\n",
       " ('systemworks', 0.036986301369863014, 0.00022893772893772894),\n",
       " ('money', 0.03972602739726028, 0.00022893772893772894)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spammiest_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spambayes', 0.0013698630136986301, 0.04601648351648352),\n",
       " ('users', 0.0013698630136986301, 0.036401098901098904),\n",
       " ('razor', 0.0013698630136986301, 0.030906593406593408),\n",
       " ('zzzzteana', 0.0013698630136986301, 0.029075091575091576),\n",
       " ('sadev', 0.0013698630136986301, 0.026785714285714284)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hammiest_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to make it quick, the same model implemented here is contained in `scikit-learn` library as `BernoulliNB` model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
